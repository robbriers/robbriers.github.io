<?xml version="1.0" encoding="utf-8" standalone="yes" ?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Rob Briers on Rob Briers</title>
    <link>/</link>
    <description>Recent content in Rob Briers on Rob Briers</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>&amp;copy; 2017 Rob Briers</copyright>
    <lastBuildDate>Wed, 20 Apr 2016 00:00:00 +0000</lastBuildDate>
    <atom:link href="/" rel="self" type="application/rss+xml" />
    
    <item>
      <title>Mapping the spread of a migratory bird using Twitter</title>
      <link>/post/2017-03-14-mapping-the-spread-of-a-migratory-bird-using-twitter/</link>
      <pubDate>Tue, 14 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-03-14-mapping-the-spread-of-a-migratory-bird-using-twitter/</guid>
      <description>&lt;p&gt;&lt;img src=&#34;/img/waxwing.jpg&#34; /&gt; I like Waxwings: big, brassy, berry-eating flockers! Living on the east coast of Scotland I should have a better than average chance of seeing them, but seem to have a knack of being at places where they are seen either just before or just after they appear.&lt;/p&gt;
&lt;p&gt;Luckily we have &lt;span class=&#34;citation&#34;&gt;@WaxwingsUK&lt;/span&gt; (&lt;a href=&#34;https://twitter.com/WaxwingsUK&#34; class=&#34;uri&#34;&gt;https://twitter.com/WaxwingsUK&lt;/a&gt;) on Twitter to report the sightings. Having watched the bumper crop of sightings unfold since last autumn I started to think about whether it would be possible to visualise the spread of records over time, using the info from the timeline of &lt;span class=&#34;citation&#34;&gt;@WaxwingsUK&lt;/span&gt;.&lt;/p&gt;
&lt;p&gt;The following is my attempt. It took more than a few train journeys to work out the details and makes use of some excellent R scripts, functions and packages that other folks have put together (acknowledged in the code that follows). In order to replicate this, you need a Twitter account and register for API keys as detailed in the links.&lt;/p&gt;
&lt;div id=&#34;stage-1-download-the-content-of-waxwingsuk-twitter-timeline-from-autumn-2016-until-present-2017-03-13.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stage 1: Download the content of &lt;span class=&#34;citation&#34;&gt;@WaxwingsUK&lt;/span&gt; Twitter timeline from autumn 2016 until present (2017-03-13).&lt;/h3&gt;
&lt;p&gt;The first part is fairly straightforward, using the TwitteR package. You will need to replace the relevant key strings with the ones that Twitter gives you when you register.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Fire up the Quattro, sorry, I mean load up the required packages
library(twitteR)
library(lubridate)
library(ggmap)
library(stringi)
library(viridis)
library(animation)

# you also need ImageMagick installed to save animations
# https://www.imagemagick.org/

# do initial setup of Twitter oauth and store in .http-oauth file
# see details at:
# http://bigcomputing.blogspot.co.uk/2016/02/the-twitter-r-package-by-jeff-gentry-is.html

consumer_key &amp;lt;- &amp;quot;your_key&amp;quot;
consumer_secret &amp;lt;- &amp;quot;your_key&amp;quot;
access_token &amp;lt;- &amp;quot;your_key&amp;quot;
access_secret &amp;lt;- &amp;quot;your_key&amp;quot;
setup_twitter_oauth(consumer_key, consumer_secret, access_token, access_secret)

# extract tweet content
waxwing_tweets &amp;lt;- userTimeline(&amp;quot;WaxwingsUK&amp;quot;, n=3200)

# set maxID to extract different parts
tweet.df &amp;lt;- twListToDF(waxwing_tweets)
all_tweets&amp;lt;-tweet.df
last_num&amp;lt;-as.numeric(tweet.df$id[nrow(tweet.df)])

# iterate through timeline, extracting each set of tweets and updating last_num
# needed due to TWitter API limits, see:
# https://dev.twitter.com/rest/public/timelines
repeat{
  waxwing_tweets &amp;lt;- userTimeline(&amp;quot;WaxwingsUK&amp;quot;, maxID=as.character(last_num), n=3200)
  tweet.df &amp;lt;- twListToDF(waxwing_tweets)
  all_tweets &amp;lt;-rbind.data.frame(all_tweets, tweet.df)
  n_recs&amp;lt;-nrow(all_tweets)
  last_num&amp;lt;-as.numeric(tweet.df$id[nrow(tweet.df)])-1
  
  # we will stop at 1000 tweets as this is more than enough
  if(n_recs&amp;gt;1000){
    break
  }
}

# now need to extract those in 2016 or 17 based on date field (created) which
# is already in POSIXct format, so should be easy using lubridate
all_tweets$Year&amp;lt;-year(all_tweets$created)
all_tweets$Month&amp;lt;-month(all_tweets$created)

# extract all 2017 tweets
records_2017&amp;lt;-all_tweets[all_tweets$Year==2017, ]

# extract 2016 tweets after April, which is the break between seasons
records_2016&amp;lt;-all_tweets[all_tweets$Year==2016 &amp;amp; all_tweets$Month&amp;gt;4, ]

# combine the records together
records&amp;lt;-rbind.data.frame(records_2017, records_2016)

# remove RTs and replies as the info is not consistently structured
# so can&amp;#39;t process easily
rts&amp;lt;-is.na(records$isRetweet)
records&amp;lt;-records[rts!=TRUE, ]
replies&amp;lt;-is.na(records$replyToSN)
records&amp;lt;-records[replies!=FALSE, ]

# just extract location string and date created columns
records&amp;lt;-cbind.data.frame(records$text, records$created)
names(records)&amp;lt;-c(&amp;quot;location&amp;quot;, &amp;quot;date&amp;quot;)

# sort records on date for subsequent plotting
records&amp;lt;- records[order(records$date),] &lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-2-text-wrangling-to-extract-the-location-and-date-of-sightings.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stage 2: Text wrangling to extract the location and date of sightings.&lt;/h3&gt;
&lt;p&gt;Luckily &lt;span class=&#34;citation&#34;&gt;@WaxwingsUK&lt;/span&gt; provides the records of sightings in a fairly consistent format, which makes life easier (nice &lt;span class=&#34;citation&#34;&gt;@WaxwingsUK&lt;/span&gt;, keep it up!). R does not have the best string manipulation functionality, but we can do it without too much hair-pulling.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# find rows with illegal characters, by conversion to control seq
illegal_chrs&amp;lt;-grepl(&amp;quot;[[:cntrl:]]&amp;quot;, stringi::stri_enc_toascii(records$location))

# remove these from the dataframe, but keep all fields
records&amp;lt;-records[illegal_chrs!=TRUE, ]

# convert to character
records$location&amp;lt;-as.character(records$location)

# split the tweet text at the : to remove the leading date info
records_locs&amp;lt;-strsplit(records$location, &amp;quot;:&amp;quot;)

# bind the records back to a df
records_locs&amp;lt;-do.call(rbind.data.frame, records_locs)

# just keep the address text, convert to character vector
records_locs&amp;lt;-as.character(records_locs[[2]])

# now split for each location, separated by ,
records_locs&amp;lt;-strsplit(records_locs, &amp;quot;,&amp;quot;)

# use length of list to determine number of records in each tweet
date_reps&amp;lt;-as.numeric(lapply(records_locs, function(x) length(x)))

# unlist records to a vector
records_locs&amp;lt;-unlist(records_locs)

# repeat dates appropriate times
dates&amp;lt;-rep(records$date, date_reps)

# some locations have more than one road/location, separated by &amp;#39;/&amp;#39;
# Google Maps API does not like these, so just use last part

# define function to split string based on last occurrence of set string
# and return the right hand portion
splitText&amp;lt;-function(x, string){
  pos&amp;lt;-gregexpr(string, x)
  posmax&amp;lt;-pos[[1]]
  # if not -1, i.e. string is found then
  if (posmax[1]!=-1){
    posmax&amp;lt;-posmax[length(posmax)]
    keep&amp;lt;-substr(x, posmax+1, nchar(x))
    return(keep)
  }
  else{
    # if the string is not found then return it unmodified
    return(x)
    }
}

# convert records_locs to dataframe for apply
records_locs&amp;lt;-as.data.frame(records_locs, stringsAsFactors = FALSE)

# process out all the &amp;#39;/&amp;#39; parts of the locations using splitText function
records_locs&amp;lt;-apply(records_locs, 1, splitText, &amp;quot;/&amp;quot;)

# split again based on &amp;#39;+&amp;#39; to remove flock size info
records_locs&amp;lt;-as.data.frame(records_locs, stringsAsFactors = FALSE)

# process out all the &amp;#39;60+ etc&amp;#39; parts of the text
records_locs&amp;lt;-apply(records_locs, 1, splitText, &amp;quot;\\+&amp;quot;)

# gsub the merry hell out of the location strings to make them suitable for 
# Google Maps API call

records_locs&amp;lt;-gsub(&amp;quot; -&amp;quot;,&amp;quot;,&amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot; \\(&amp;quot;, &amp;quot;, &amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot;\\)&amp;quot;, &amp;quot;, UK&amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot;@&amp;quot;, &amp;quot;&amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot;_&amp;quot;, &amp;quot; &amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot;/&amp;quot;, &amp;quot; &amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot; Dr &amp;quot;, &amp;quot; Drive &amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot; Rd &amp;quot;, &amp;quot; Road &amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot; Ln &amp;quot;, &amp;quot; Lane&amp;quot; , records_locs)
records_locs&amp;lt;-gsub(&amp;quot; Crt &amp;quot;, &amp;quot; Court &amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot; St &amp;quot;, &amp;quot; Street &amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot; Cl &amp;quot;, &amp;quot; Close &amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot; car park&amp;quot;, &amp;quot;&amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot; opp&amp;quot;, &amp;quot;&amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot; private garden&amp;quot;, &amp;quot;&amp;quot;, records_locs)
records_locs&amp;lt;-gsub(&amp;quot; &amp;amp;amp;&amp;quot;, &amp;quot; and&amp;quot;, records_locs)

# trim whitespace, now ready for the geocoding stage
records_locs&amp;lt;-trimws(records_locs)&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-3-geocoding-the-sightings.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stage 3: Geocoding the sightings.&lt;/h3&gt;
&lt;p&gt;This makes use of the &lt;code&gt;geocode&lt;/code&gt; function in the excellent &lt;code&gt;ggmap&lt;/code&gt; package. Even better Shane Lynn has written a function that allows you to ping the Google Maps API up to the public use limit (2500 per day) and pause until you can do some more. We only have about 1600 locations so can do it in one go.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# Function to geocode addresses via Google Maps API, by Shane Lynn
# http://www.shanelynn.ie/massive-geocoding-with-r-and-google-maps/

# define a function that will process googles server responses for us.
getGeoDetails &amp;lt;- function(address){
  # use the gecode function to query google servers
  geo_reply = geocode(address, output=&amp;#39;all&amp;#39;, messaging=TRUE, override_limit=TRUE)
  # now extract the bits that we need from the returned list
  answer &amp;lt;- data.frame(lat=NA, long=NA, accuracy=NA, formatted_address=NA, address_type=NA, status=NA)
  answer$status &amp;lt;- geo_reply$status
  
  # if we are over the query limit - want to pause for an hour
  while(geo_reply$status == &amp;quot;OVER_QUERY_LIMIT&amp;quot;){
    print(&amp;quot;OVER QUERY LIMIT - Pausing for 1 hour at:&amp;quot;) 
    time &amp;lt;- Sys.time()
    print(as.character(time))
    Sys.sleep(60*60)
    geo_reply = geocode(address, output=&amp;#39;all&amp;#39;, messaging=TRUE, override_limit=TRUE)
    answer$status &amp;lt;- geo_reply$status
  }
  
  # return Na&amp;#39;s if we didn&amp;#39;t get a match:
  if (geo_reply$status != &amp;quot;OK&amp;quot;){
    return(answer)
  }
  # else, extract what we need from the Google server reply into a dataframe:
  answer$lat &amp;lt;- geo_reply$results[[1]]$geometry$location$lat
  answer$long &amp;lt;- geo_reply$results[[1]]$geometry$location$lng
  if (length(geo_reply$results[[1]]$types) &amp;gt; 0){
    answer$accuracy &amp;lt;- geo_reply$results[[1]]$types[[1]]
  }
  answer$address_type &amp;lt;- paste(geo_reply$results[[1]]$types, collapse=&amp;#39;,&amp;#39;)
  answer$formatted_address &amp;lt;- geo_reply$results[[1]]$formatted_address
  
  return(answer)
}

# initialise a dataframe to hold the results
geocoded &amp;lt;- data.frame()
# find out where to start in the address list (if the script was interrupted before):
startindex &amp;lt;- 1
# if a temp file exists - load it up and count the rows!
tempfilename &amp;lt;- &amp;#39;temp_geocoded.rds&amp;#39;
if (file.exists(tempfilename)){
  print(&amp;quot;Found temp file - resuming from index:&amp;quot;)
  geocoded &amp;lt;- readRDS(tempfilename)
  startindex &amp;lt;- nrow(geocoded)
  print(startindex)
}

# Start the geocoding process - address by address. geocode() function takes care of query speed limit.
for (ii in seq(startindex, length(records_locs))){
  print(paste(&amp;quot;Working on index&amp;quot;, ii, &amp;quot;of&amp;quot;, length(records_locs)))
  # query the google geocoder - this will pause here if we are over the limit.
  result = getGeoDetails(records_locs[ii]) 
  print(result$status)
  result$index &amp;lt;- ii
  # append the answer to the results file.
  geocoded &amp;lt;- rbind(geocoded, result)
  # save temporary results as we are going along
  saveRDS(geocoded, tempfilename)
}

# geocoded now contains the lat and long (plus other things)
# of the records, in date order, so combine with other info
plotdata&amp;lt;-cbind.data.frame(records_locs, dates, geocoded$long, geocoded$lat)
names(plotdata)&amp;lt;-c(&amp;quot;location&amp;quot;, &amp;quot;date&amp;quot;, &amp;quot;long&amp;quot;, &amp;quot;lat&amp;quot;)

# remove those records that did not get geocoded (NA)
noloc&amp;lt;-is.na(plotdata$long)
plotdata&amp;lt;-plotdata[noloc!=TRUE, ]

# some geocoding results are not in the UK despite best efforts so
# need to exclude these, using UK lat/long limits (only lower lat value
# needed in this case)
plotdata&amp;lt;-plotdata[plotdata$lat&amp;gt;49, ]

# we have lost 272 records on the way through, through no/wrong geocoding
# that is not too bad really, still got ~1400&lt;/code&gt;&lt;/pre&gt;
&lt;/div&gt;
&lt;div id=&#34;stage-4-map-and-animate-the-sightings.&#34; class=&#34;section level3&#34;&gt;
&lt;h3&gt;Stage 4: Map and animate the sightings.&lt;/h3&gt;
&lt;p&gt;Again based around the functions of &lt;code&gt;ggmap&lt;/code&gt;, plus the &lt;code&gt;animation&lt;/code&gt; package. You need to install &lt;a href=&#34;http://www.imagemagick.org/script/index.php&#34;&gt;ImageMagick&lt;/a&gt; first as well. Plotting the sightings over time and grading the symbol colour by the date of record using the plasma scheme from the &lt;code&gt;viridis&lt;/code&gt; package as this shows up nicely against the Google Maps background. The &lt;code&gt;saveVideo&lt;/code&gt; function at the end outputs to an mp4 called animation.mp4 in the working directory.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# set up base map for plotting, setting centre long/lat
myLocation &amp;lt;- c(-2, 54.5)

# using Google roadmap as the base
waxMap &amp;lt;- get_map(location=myLocation, zoom=5, source=&amp;quot;google&amp;quot;, maptype=&amp;quot;roadmap&amp;quot;, crop=FALSE)

# set up colours for points, based on viridis plasma scheme
plotdata$colours&amp;lt;-plasma(1395, begin=0, end =0.9)

# set up function to plot map in sequence
plotmaps &amp;lt;- function(){
    for (m in seq(1, nrow(plotdata))){
      points&amp;lt;-plotdata[1:m, ]
      p&amp;lt;-ggmap(waxMap)+
        geom_point(aes(x = long, y = lat), data = points, alpha = 1, color=points$colours, size = 2)+
        scale_x_continuous(limits = c(-11, 4), expand = c(0, 0)) +
        scale_y_continuous(limits = c(49, 61), expand = c(0, 0))
    print(p)
  }
}  

# set video options
oopt = ani.options(interval = 0.05, nmax = nrow(plotdata))  

# output video; this will throw a series of warnings over
# changing scales of axes, but it is all good
saveVideo(plotmaps(),interval = 0.05)&lt;/code&gt;&lt;/pre&gt;
&lt;div style=&#34;position:relative;padding-bottom:56%;padding-top:20px;height:0;&#34;&gt;
&lt;iframe src=&#34;https://onlinevideo.napier.ac.uk/player?autostart=n&amp;amp;fullscreen=y&amp;amp;width=0&amp;amp;height=0&amp;amp;videoId=9832&amp;amp;quality=hi&amp;amp;captions=n&amp;amp;chapterId=0&#34; frameborder=&#34;0&#34; scrolling=&#34;no&#34; style=&#34;position:absolute;top:0;left:0;width:100%;height:100%;&#34; allowfullscreen&gt;
&lt;/iframe&gt;
&lt;/div&gt;
&lt;p&gt;We can also do a static version as well as video.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;p&amp;lt;-ggmap(waxMap)+
  geom_point(aes(x = long, y = lat), data = plotdata, alpha = 1, color=plotdata$colours, size = 2)+
  scale_x_continuous(limits = c(-11, 4), expand = c(0, 0)) +
  scale_y_continuous(limits = c(49, 61), expand = c(0, 0))
p&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-03-14-mapping-the-spread-of-a-migratory-bird-using-twitter_files/figure-html/unnamed-chunk-6-1.png&#34; width=&#34;672&#34; /&gt; Waxwing image CC0 &lt;a href=&#34;https://pixabay.com/en/waxwing-bird-rowan-rowan-berries-1163294/&#34;&gt;gustavmelin0&lt;/a&gt;.&lt;/p&gt;
&lt;/div&gt;
</description>
    </item>
    
    <item>
      <title>Is research becoming more collaborative?</title>
      <link>/post/is-research-becoming-more-collaborative/</link>
      <pubDate>Mon, 06 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/is-research-becoming-more-collaborative/</guid>
      <description>&lt;p&gt;I had an interesting discussion recently regarding the extent to which research is becoming a more collaborative enterprise. Gone is the image of the lone pioneer, locked away in the lab and wrestling with the world until the key advance is reached (Health and Safety would have a field day with this anyway!). With the ease of communication, sharing of data and analyses and other developments, it is far from suprising that there have been a growth in the extent to which collaboration has become the norm (see also evidence from Jonathan Adams in &lt;a href=&#34;http://www.nature.com/nature/journal/v490/n7420/full/490335a.html&#34;&gt;Nature&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;On a more personal level, just thinking back over the time that I have been involved in research (which shockingly has now been 20 years!), I had the impression that the extent to which I have collaborated with a range of colleagues has increased over time. As a PhD student, my collaboration in terms of publication was largely limited to my supervisor, although I suspect that nowadays the range of individuals that students collaborate with is getting wider as well). Moving on to a post-doc, my collaborators started to widen, both within the project that I was employed on, and a number of ‘side-projects’ that resulted in publications too. Since taking up a full-time position in academia, the opportunities for collaboration and research are gradually broadened as well, with opportunities for much more diverse (some might say too diverse, showing a lack of focus!) projects, interactions with research students and research staff both internal and external. This more personal trajectory is slightly different from broader trends in research but I suspect is also a fairly common pattern for staff in academic institutions.&lt;/p&gt;
&lt;p&gt;Anyway, never one to let an impression get in the way of doing some number crunching, I decided to do some analysis of temporal change in my publication coauthorship, as a proxy for collaboration. I could have used the number of institutions involved as an alternative, but that underweights internal collaboration within my institution, which has been signficant for me at least.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# analysis of changes in number of co-authors over time, based on my publication history

# load the libraries needed, we will use viridis to colour the plot symbols
library(ggplot2)
library(viridis)

# read in the data direct from the Github repo - you can do it too if you want!
auths&amp;lt;-read.csv(&amp;#39;https://raw.githubusercontent.com/robbriers/authors/master/coauthors.csv&amp;#39;, header=TRUE)

# plot the data to look at the trend, using poisson GLM for fitted line
p1 &amp;lt;- ggplot(auths, aes(x = year, y = authors, colour = authors)) +
  geom_jitter(size=1.5) +
  scale_color_viridis(end = 0.80) +
  labs(title = &amp;quot;Change in number of co-authors over time&amp;quot;,
       x = &amp;quot;year of publication&amp;quot;,
       y = &amp;quot;number of co-authors&amp;quot;) +
  geom_smooth(method = &amp;quot;glm&amp;quot;, method.args = list(family = &amp;quot;poisson&amp;quot;), se = TRUE) +
  theme_bw() +
  annotate(&amp;quot;text&amp;quot;, x=2006, y=14, label=&amp;quot;Spot the multi-author workshop paper&amp;quot;) +
  theme(legend.position=&amp;quot;none&amp;quot;)

# show the plot
p1&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-03-06-is-research-becoming-more-collaborative_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt; So a definite upward trend in collaboration over time. To be more specific, we can fit the poisson model directly.&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# fit a poisson glm to the pattern of change, so we can make some predictions
auth.m1&amp;lt;-glm(authors~year, data=auths, family=&amp;quot;poisson&amp;quot;)
summary(auth.m1)&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;## 
## Call:
## glm(formula = authors ~ year, family = &amp;quot;poisson&amp;quot;, data = auths)
## 
## Deviance Residuals: 
##     Min       1Q   Median       3Q      Max  
## -2.2282  -0.6275  -0.3446   0.3353   4.8714  
## 
## Coefficients:
##               Estimate Std. Error z value Pr(&amp;gt;|z|)    
## (Intercept) -126.86415   33.11267  -3.831 0.000127 ***
## year           0.06380    0.01649   3.870 0.000109 ***
## ---
## Signif. codes:  0 &amp;#39;***&amp;#39; 0.001 &amp;#39;**&amp;#39; 0.01 &amp;#39;*&amp;#39; 0.05 &amp;#39;.&amp;#39; 0.1 &amp;#39; &amp;#39; 1
## 
## (Dispersion parameter for poisson family taken to be 1)
## 
##     Null deviance: 59.277  on 34  degrees of freedom
## Residual deviance: 44.527  on 33  degrees of freedom
## AIC: 149.9
## 
## Number of Fisher Scoring iterations: 5&lt;/code&gt;&lt;/pre&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# so how many co-authors will I have in 2040, assuming I make it?
exp(predict(auth.m1, data.frame(year=2040)))&lt;/code&gt;&lt;/pre&gt;
&lt;pre&gt;&lt;code&gt;##        1 
## 26.86821&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;Wow, really looking forward to the ‘Track Changes’ on that manuscript!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>A new website</title>
      <link>/post/2017-03-01-a-new-website/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/post/2017-03-01-a-new-website/</guid>
      <description>&lt;p&gt;So I finally got round to creating a new website. It has taken a fair while, as there is always something more pressing to do. The world of websites has moved on somewhat since my last homestyle HTML affair. I experimented with &lt;a href=&#34;https://www.wordpress.com&#34;&gt;Wordpress&lt;/a&gt; for a while, but was not keen on the limitations imposed. In the meantime a range of nice static website generators appeared.&lt;/p&gt;
&lt;p&gt;As the site is hosted on Github pages, I tried out &lt;a href=&#34;http://jekyllrb.com/&#34;&gt;Jekyll&lt;/a&gt;, but at the same time I came across the &lt;a href=&#34;https://github.com/blogdown/blogdown&#34;&gt;&lt;code&gt;blogdown&lt;/code&gt;&lt;/a&gt; package for R, which is based on the &lt;a href=&#34;https://gohugo.io/&#34;&gt;Hugo&lt;/a&gt; static site generator. This had the double advantage of allowing me to write in Markdown or Rmarkdown, simplifying content generation, and also being able to manage content within &lt;a href=&#34;https://www.rstudio.com&#34;&gt;RStudio&lt;/a&gt;, where I do all my data analysis and graphics anyway. So not only is writing content easy, I can also easily add in &lt;code&gt;R&lt;/code&gt; code and plots.&lt;/p&gt;
&lt;p&gt;To demonstrate, let’s do a quick bit of plotting…&lt;/p&gt;
&lt;pre class=&#34;r&#34;&gt;&lt;code&gt;# load up the classic &amp;#39;mite&amp;#39; data from the vegan package and ggplot2
library(vegan)
library(ggplot2)
data(mite)
data(mite.xy)

# bind the x and y coordinates with the TVEL abundance data
df&amp;lt;-cbind.data.frame(mite$TVEL, mite.xy$x, mite.xy$y)

# rename the columns
colnames(df)&amp;lt;-c(&amp;quot;TVEL&amp;quot;, &amp;quot;x.coord&amp;quot;, &amp;quot;y.coord&amp;quot;)

# produce a simple bubble plot of the abundances across space
ggplot(df, aes(x=x.coord, y=y.coord)) +
  geom_point(shape=1,color=&amp;quot;red&amp;quot;, size=df$TVEL) +
  theme_bw()&lt;/code&gt;&lt;/pre&gt;
&lt;p&gt;&lt;img src=&#34;/post/2017-03-01-a-new-website_files/figure-html/unnamed-chunk-1-1.png&#34; width=&#34;672&#34; /&gt; Nice!&lt;/p&gt;
&lt;p&gt;Documentation for the package is a bit sparse, but I found some really useful advice at &lt;a href=&#34;https://proquestionasker.github.io/blog/Making_Site/&#34; class=&#34;uri&#34;&gt;https://proquestionasker.github.io/blog/Making_Site/&lt;/a&gt;.&lt;/p&gt;
&lt;p&gt;There are a huge range of site themes available for Hugo, which you can install using &lt;code&gt;blogdown&lt;/code&gt;. I went with the &lt;a href=&#34;https://github.com/gcushen/hugo-academic&#34;&gt;Academic Theme&lt;/a&gt; and subsequently learnt how little I know about CSS, let alone YAML/TOML! Still, I got more or less sorted in the end. Have a look around - it’s still a bit of a work in progress, but the majority of things work now.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Urban ponds as an aquatic biodiversity resource in modified landscapes</title>
      <link>/publication/urban_ponds/</link>
      <pubDate>Wed, 01 Mar 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/urban_ponds/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Evaluating, predicting and mapping belowground carbon stores in Kenyan mangroves</title>
      <link>/publication/mangrove_carbon/</link>
      <pubDate>Sun, 01 Jan 2017 00:00:00 +0000</pubDate>
      
      <guid>/publication/mangrove_carbon/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Anthropogenic impacts</title>
      <link>/project/impacts/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/impacts/</guid>
      <description>

&lt;p&gt;&lt;img src=&#34;/img/spectrogram.png&#34; alt=&#34;spectrogram&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;underwater-noise-effects&#34;&gt;Underwater noise effects&lt;/h3&gt;

&lt;p&gt;Increasing levels of underwater noise resulting from anthropogenic sources are an emerging source of pollution. I have recently become interested in the effects of this on invertebrates in marine ecosystems. This work is being undertaken in conjunction with &lt;a href=&#34;http://www.napier.ac.uk/people/karen-diele&#34; target=&#34;_blank&#34;&gt;Dr Karen Diele&lt;/a&gt; and other collaborators, with some of the practical work being undertaken at the &lt;a href=&#34;http://www.marinestation.co.uk&#34; target=&#34;_blank&#34;&gt;St Abbs Marine Station&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/halbeath.png&#34; alt=&#34;halbeath pond&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;diffuse-pollution-and-urban-drainage&#34;&gt;Diffuse pollution and urban drainage&lt;/h3&gt;

&lt;p&gt;Diffuse pollution is a widespread problem in freshwater ecosysterms and my work has involved examining and predicting the impacts of diffuse pollution in freshwaters and evaluating the role of constructed wetlands (SUDS and similar approaches in rural areas) in alleviating these issues and supporting biodiversity goals. See &lt;a href=&#34;/publication/suds/&#34;&gt;Briers 2014&lt;/a&gt;.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Coding</title>
      <link>/project/coding/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/coding/</guid>
      <description>

&lt;h3 id=&#34;r-code&#34;&gt;R code&lt;/h3&gt;

&lt;p&gt;I am the creator and maintainer of the &lt;code&gt;biotic&lt;/code&gt; package for R. This calculates a range of common UK freshwater biotic indicies. The stable version (0.1.2) can be installed from &lt;a href=&#34;https://cran.r-project.org/package=biotic&#34; target=&#34;_blank&#34;&gt;CRAN&lt;/a&gt;. I am currently starting work on version 0.2. Follow progress, request a feature, log an issue or install the development version from &lt;a href=&#34;https://github.com/robbriers/biotic&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;python&#34;&gt;Python&lt;/h3&gt;

&lt;p&gt;I use GIS in a lot of my work, mainly &lt;a href=&#34;http://www.esri.com/arcgis/about-arcgis&#34; target=&#34;_blank&#34;&gt;ArcGIS&lt;/a&gt; for my sins, but I am trying to wean myself off and move over to a combination of &lt;a href=&#34;https://www.qgis.org/&#34; target=&#34;_blank&#34;&gt;QGIS&lt;/a&gt; and &lt;a href=&#34;https://www.r-project.org/&#34; target=&#34;_blank&#34;&gt;R&lt;/a&gt;. I have used Python to develop some scripts and tools for various analyses, such as randomisation tests to determine the extent of clustering of marine invasive species around ports, import of data from the National Biodiversity Network (before they improved their output data format) and top-of-the atmosphere correction of Landsat images. Most of this is either too hacky to release or has been superceded by other tools. If I ever get round to sorting it out, then it will be made available on &lt;a href=&#34;https://github.com/robbriers&#34; target=&#34;_blank&#34;&gt;Github&lt;/a&gt;.&lt;/p&gt;

&lt;h3 id=&#34;visual-basic-for-applications&#34;&gt;Visual Basic for Applications&lt;/h3&gt;

&lt;p&gt;I have developed a few add-ins for Excel using VBA. You can download these (zipped add-in files with outline instructions) below:&lt;/p&gt;

&lt;p&gt;&lt;i class=&#34;fa fa-code&#34;&gt;&lt;/i&gt; &lt;a href=&#34;/code/Depletion.zip&#34;&gt;Depletion.xla&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This calculates population estimates using the Carle &amp;amp; Strube and Zippen estimators&lt;/p&gt;

&lt;p&gt;&lt;i class=&#34;fa fa-code&#34;&gt;&lt;/i&gt; &lt;a href=&#34;/code/BMWP.zip&#34;&gt;BMWP.xla&lt;/a&gt;&lt;/p&gt;

&lt;p&gt;This calculates BMWP and associated indices for UK freshwater invertebrate samples. Now superceded by the &lt;code&gt;biotic&lt;/code&gt; package above, but available for posterity!&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Conservation planning</title>
      <link>/project/conservation/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/conservation/</guid>
      <description>

&lt;p&gt;I have had a long-standing interest in applying formal optimisation methods in the evaluation of priorities for conservation in relation to protected area planning.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/fife_birds.png&#34; alt=&#34;fife birds&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;spatial-methods-of-protected-area-planning&#34;&gt;Spatial methods of protected area planning&lt;/h3&gt;

&lt;p&gt;This work initially focused largely on terrestrial ecosystems, and specifically the development of approaches to site selection that involved consideration of spatial criteria. The spatial configuration of protected sites is an important consideration for species persistence and I developed both heuristic and integer programming-based methods for incorporating this, in conjunction with &lt;a href=&#34;http://ace.illinois.edu/directory/h-onal&#34; target=&#34;_blank&#34;&gt;Prof Hayri Onal&lt;/a&gt;. For example papers, see &lt;a href=&#34;/publication/connectivity/&#34;&gt;Briers 2002&lt;/a&gt;, &lt;a href=&#34;/publication/connected/&#34;&gt;Onal &amp;amp; Briers 2006&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/salmon_farm.png&#34; alt=&#34;salmon farm&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;marine-spatial-planning&#34;&gt;Marine spatial planning&lt;/h3&gt;

&lt;p&gt;More recently I have developed an interest in marine spatial planning (MSP) developments. Marine space is complex and commonly multi-use, requiring different approaches to the designation of space. See for example &lt;a href=&#34;/publication/zoning_scheme/&#34;&gt;McWhinnie et al 2015&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/mangroves.png&#34; alt=&#34;mangroves&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;mangrove-protection&#34;&gt;Mangrove protection&lt;/h3&gt;

&lt;p&gt;I have also begun work to combine formal optimisation methods with data on carbon stores and risk factors in mangrove ecoystems to look at approaches to prioritisation for protection and restoration. This is linked to empirical and modelling work detailed in the &lt;a href=&#34;/project/spatial-ecology/&#34;&gt;Spatial Ecology&lt;/a&gt; section.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Spatial Ecology</title>
      <link>/project/spatial-ecology/</link>
      <pubDate>Wed, 27 Apr 2016 00:00:00 +0000</pubDate>
      
      <guid>/project/spatial-ecology/</guid>
      <description>

&lt;p&gt;I am interested in how populations and communities vary and interact across space. This work is wide ranging and covers both field-based and modelling approaches, often using GIS.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/trout.png&#34; alt=&#34;trout&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;stable-isotope-ecology-of-salmonids&#34;&gt;Stable isotope ecology of salmonids&lt;/h3&gt;

&lt;p&gt;I am currently working with &lt;a href=&#34;http://www.tweedfoundation.org.uk&#34; target=&#34;_blank&#34;&gt;The Tweed Foundation&lt;/a&gt;, &lt;a href=&#34;http://deveron.org/&#34; target=&#34;_blank&#34;&gt;The Deveron, Isla and Bogie Charitable Trust&lt;/a&gt; and other partners using stable isotope approaches to examine the distribution of migratory and non-migratory trout reproduction across river systems. Distinction in stable isotope (C and N) characteristics allows reproduction derived from the different forms to be distinguished at the fry stage (see &lt;a href=&#34;/publication/carotenoids/&#34;&gt;Briers et al 2013&lt;/a&gt;), but there is a strong influence of catchment land-use on the rate and extent of temporal change post hatching, which we are currently investigating.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/rhizophora.png&#34; alt=&#34;mangroves&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;mangrove-carbon-storage-and-management&#34;&gt;Mangrove carbon storage and management&lt;/h3&gt;

&lt;p&gt;In conjunction with &lt;a href=&#34;http://www.napier.ac.uk/people/mark-huxham&#34; target=&#34;_blank&#34;&gt;Prof Mark Huxham&lt;/a&gt; here in the School, colleagues from &lt;a href=&#34;https://ecometrica.com&#34; target=&#34;_blank&#34;&gt;Ecometrica&lt;/a&gt; and &lt;a href=&#34;http://www.kmfri.co.ke&#34; target=&#34;_blank&#34;&gt;KMFRI&lt;/a&gt;, I have been undertaking research combining field survey with GIS and spatial statistical techniques examine variation in carbon storage in mangrove ecosystems, rates of loss drivers of change. Example papers include &lt;a href=&#34;/publication/mangrove_carbon/&#34;&gt;Gress et al 2017&lt;/a&gt; and &lt;a href=&#34;/publication/mangrove_risk/&#34;&gt;Rideout et al 2013&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/pond4.png&#34; alt=&#34;gam model&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;wonderful-ponds&#34;&gt;Wonderful ponds!&lt;/h3&gt;

&lt;p&gt;Ponds are great systems for addressing a whole range of ecological questions and I have a long-standing interest in patterns in population and community structure of pond dwelling organisms (mostly invertebrates) across space, along with assessment of their conservation value. See for example &lt;a href=&#34;/publication/urban_ponds/&#34;&gt;Hill et al &lt;em&gt;in press&lt;/em&gt;&lt;/a&gt;, &lt;a href=&#34;/publication/spatial_pond/&#34;&gt;Briers &amp;amp; Biggs 2005&lt;/a&gt; and &lt;a href=&#34;/publication/metapop/&#34;&gt;Briers &amp;amp; Warren 2000&lt;/a&gt;.&lt;/p&gt;

&lt;p&gt;&lt;img src=&#34;/img/gam.png&#34; alt=&#34;gam model&#34; /&gt;&lt;/p&gt;

&lt;h3 id=&#34;other-areas&#34;&gt;Other areas&lt;/h3&gt;

&lt;p&gt;Other current or recent relevant work has focused on diverse topics including spatio-temporal patterns in marine debris deposition, lynx habitat selection and spatial patterns in host-parasite associations.&lt;/p&gt;
</description>
    </item>
    
    <item>
      <title>Eurasian lynx natal den site and maternal home-range selection in multi-use landscapes of Norway</title>
      <link>/publication/lynx_dens/</link>
      <pubDate>Thu, 01 Oct 2015 00:00:00 +0000</pubDate>
      
      <guid>/publication/lynx_dens/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Applying Climate Compatible Development and economic valuation to coastal management: A case study of Kenya&#39;s mangrove forests</title>
      <link>/publication/mangrove_ccd/</link>
      <pubDate>Wed, 01 Jul 2015 00:00:00 +0000</pubDate>
      
      <guid>/publication/mangrove_ccd/</guid>
      <description></description>
    </item>
    
    <item>
      <title>The development and testing of a multiple-use zoning scheme for Scottish waters</title>
      <link>/publication/zoning_scheme/</link>
      <pubDate>Thu, 01 Jan 2015 00:00:00 +0000</pubDate>
      
      <guid>/publication/zoning_scheme/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Structural and functional indices show similar performance in marine ecosystem quality assessment</title>
      <link>/publication/indicators/</link>
      <pubDate>Fri, 01 Aug 2014 00:00:00 +0000</pubDate>
      
      <guid>/publication/indicators/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Invertebrate Communities and Environmental Conditions in a Series of Urban Drainage Ponds in Eastern Scotland: Implications for Biodiversity and Conservation Value of SUDS</title>
      <link>/publication/suds/</link>
      <pubDate>Sat, 01 Feb 2014 00:00:00 +0000</pubDate>
      
      <guid>/publication/suds/</guid>
      <description></description>
    </item>
    
    <item>
      <title>Making predictions of mangrove deforestation: a comparison of two methods in Kenya</title>
      <link>/publication/mangrove_risk/</link>
      <pubDate>Fri, 01 Nov 2013 00:00:00 +0000</pubDate>
      
      <guid>/publication/mangrove_risk/</guid>
      <description></description>
    </item>
    
  </channel>
</rss>
